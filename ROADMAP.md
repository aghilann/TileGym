<!--- SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved. --->

<!--- SPDX-License-Identifier: MIT --->

# ğŸ—ºï¸ TileGym Kernel Roadmap & Contribution Guide

Welcome to the TileGym roadmap! We use this page to provide transparency into our development progress and to invite the community to help us build the next generation of tile-based high-performance kernels.

## 1. Current Support Status

### 1.1 Operator Support

The following table tracks the support status for various operators.

| Category | Operator | Forward | Backward |
|----------|----------|--------|----------|
| Linear Algebra | MatMul | âœ… Available | N/A |
| Linear Algebra | Batch MatMul (BMM) | âœ… Available | ğŸ“… Planned |
| Linear Algebra | Grouped GEMM | âœ… Available | N/A |
| Linear Algebra | FP8 Quantized MatMul | ğŸš§ WIP (Internal) | N/A |
| Linear Algebra | Split-K Reduction | âœ… Available | N/A |
| Attention | Attention | âœ… Available | ğŸ“… Planned |
| Attention | Flash Decode | âœ… Available | N/A |
| Attention | Attention Sink Decode | ğŸš§ WIP (Internal) | N/A |
| Attention | Attention Sink | ğŸ“… Planned | N/A |
| Attention | Autoregressive Flash Attention | ğŸ“… Planned | N/A |
| Attention | Flex Attention | ğŸ“… Planned | N/A |
| Attention | Multi-Head Compression (MHC) | âœ… Available | N/A |
| Attention | Multi-Latent Attention (MLA) | âœ… Available | N/A |
| Attention | MLA Decoding | âœ… Available | N/A |
| Attention | MLA Decoding Split KV | âœ… Available | N/A |
| Normalization | RMS Normalization | âœ… Available | âœ… Available |
| Normalization | Layer Normalization Legacy | âœ… Available | ğŸ“… Planned |
| Normalization | Cache Layer Normalization | ğŸš§ WIP (Internal) | ğŸš§ WIP (Internal) |
| Normalization | Group Normalization | ğŸ“… Planned | N/A |
| Activation | SiLU and Mul | âœ… Available | ğŸ™‹ Help Wanted |
| Activation | SwiGLU | âœ… Available | ğŸ“… Planned |
| Activation | Dropout | âœ… Available | N/A |
| Activation | Softmax | âœ… Available | ğŸš§ WIP (Internal) |
| Fused Operations | Linear + Activation + Linear | ğŸš§ WIP (Internal) | ğŸš§ WIP (Internal) |
| Fused Operations | Linear + Bias + Activation | ğŸš§ WIP (Internal) | ğŸš§ WIP (Internal) |
| Fused Operations | Linear + Elementwise | ğŸš§ WIP (Internal) | N/A |
| Fused Operations | Linear + GLU Activation + Linear | ğŸš§ WIP (Internal) | ğŸ“… Planned |
| Mixture of Experts | MoE | âœ… Available | N/A |
| Mixture of Experts | MoE Align Block | âœ… Available | N/A |
| Positional Encoding | Rotary Position Embedding (RoPE) | âœ… Available | ğŸ“… Planned |
| Tensor Manipulation | Concatenation | ğŸš§ WIP (Internal) | N/A |
| Tensor Manipulation | Transpose | ğŸš§ WIP (Internal) | N/A |
| Signal Processing | Fast Fourier Transform (FFT) | ğŸš§ WIP (Internal) | N/A |
| Convolution | Convolution | ğŸ“… Planned | ğŸ“… Planned |
| Loss Functions | Cross Entropy | ğŸ“… Planned | ğŸ“… Planned |
| Embedding | BERT Embeddings | ğŸ“… Planned | N/A |
| Optimizer | Fused Adam | ğŸ“… Planned | N/A |
| Pointwise | Squares | ğŸ“… Planned | N/A |

### 1.2 E2E Model Support

The following table tracks the support status for various models.

| Model | Status | Notes |
|-------|--------|-------|
| Llama 3.1 | âœ… Available | Tested in B200 |
| DeepseekV2-Litechat | âœ… Available | Tested in B200 |
| Qwen-2 | âœ… Available | Tested in B200 |
| GPT-OSS Gemma-3 | ğŸš§ WIP (Internal) | |
| More LLM models | ğŸ™‹ Help Wanted | |

### 1.3 Kernel Library Support

The following table tracks the support status for various kernel libraries.

| Library | Status | Notes |
|---------|--------|-------|
| Flashinfer | ğŸš§ WIP (Internal) | |
| Tokamax | ğŸš§ WIP (Internal) | |
| Flaggems | ğŸš§ WIP (Internal) |  |
| Other Libraries | ğŸ“… Planned | We welcome suggestions on which repositories you'd like to see cuTile performance in |

### Status Definitions:

- **âœ… Available**: Fully tested, performance optimized, and ready for production use.
- **ğŸš§ WIP (Internal)**: Currently being developed by the NVIDIA team. (Internal development is active; we recommend waiting for our PR to avoid conflicts).
- **ğŸ“… Planned**: On our radar for future development. We are open to design discussions.
- **ğŸ™‹ Help Wanted**: We would love to have this, but don't have the bandwidth yet. Community contributions are highly encouraged!

## 2. Contribution Opportunities

We are actively looking for contributors to help with the following strategic areas:

### ğŸš€ Kernel Implementations (High Priority)

#### Optimize Existing Kernels

Make existing kernels run faster. Our internal optimization efforts currently focus on B200. If you discover optimizations that can make kernels faster, we welcome your contributions. You can choose to add tuning configs for specific architectures. However, if you make changes to the kernel itself, we will internally test whether your optimizations cause performance regressions on all covered GPUs.

#### Submit New Kernels

We welcome contributions of any new kernels, especially kernels required by new models. Before you start implementing, please check existing kernels in the repository, review our roadmap, and search through open issues to ensure that no one else is already working on the same kernel.

### ğŸ”— E2E Model Support

**New Model Integration**: Help us support more LLM models (e.g., Mixtral, Llama 4 and beyond).

**Model Optimization**: Performance tuning and optimization for existing model support.


## 3. How to Contribute

For detailed contribution guidelines, please refer to [CONTRIBUTING.md](CONTRIBUTING.md).

If you want to contribute a new kernel or claim a Help Wanted task:

1. **Review Existing Code**: Check `tilegym/ops/cutile` (e.g., the GEMM implementation) to understand our DSL and coding standards.

2. **Submit a PR**: Directly open a pull request with your implementation. Your PR description must include:
   - Performance profiling data comparing against baseline implementations (e.g., torch, cuBLAS, flashinfer, or Triton).
   - Unit tests covering various shapes.

**For E2E Model Support**: If your contribution involves end-to-end model support and will take a significant amount of time, please open an issue first to discuss your approach and let us know that you are working on it. This helps us coordinate efforts and avoid duplicate work.

If you meet any problems, please [Open an Issue] to let us know. Your feedback helps us prioritize our internal roadmap!
